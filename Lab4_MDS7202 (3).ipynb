{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac9155b9f5e04400957a6f8bb3f6610c",
        "deepnote_cell_type": "markdown",
        "id": "2v2D1coL7I8i"
      },
      "source": [
        "<h1><center>Laboratorio 4: La solicitud de Mathias ü§ó</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d3d6f6d405c54dbe985a5f4b3e4f9120",
        "deepnote_cell_type": "markdown",
        "id": "YxdTmIPD7L_x"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "851a7788e8214942863cbd4099064ab2",
        "deepnote_cell_type": "markdown",
        "id": "Y2Gyrj-x7N2L"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Camila Vera Gallardo\n",
        "- Nombre de alumno 2: Sebastian Calderon Altamirano\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f23a189afdec4e198683308db70e43b7",
        "deepnote_cell_type": "markdown",
        "id": "jQ9skYc57Pxi"
      },
      "source": [
        "### **Link de repositorio de GitHub:**\n",
        "\n",
        "[Repositorio Sebastian Calderon](https://github.com/eldiddy/eldiddy.git)\n",
        "\n",
        "[Repositorio Camila Vera](https://github.com/cxmilavg/cxmilavg.git)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5318f41cda64d4290a7a548956ed725",
        "deepnote_cell_type": "markdown",
        "id": "1M4PoEWm7S80"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers.\n",
        "- Utilizar diferentes algoritmos de cluster y ver el desempe√±o.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar clusters.\n",
        "- Familiarizarse con plotly.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "858df483d9e64780a21674afed1d34b8",
        "deepnote_cell_type": "markdown",
        "id": "SuMbiyQZG2Cc"
      },
      "source": [
        "## Descripci√≥n del laboratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "403ffe48ec994afda4b91e670a08d0ef",
        "deepnote_cell_type": "markdown",
        "id": "QZsNO4rUrqCz"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5a/a6/af/5aa6afde8490da403a21601adf7a7240.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0303baa17d4546feae8c9b88c58470bf",
        "deepnote_cell_type": "markdown",
        "id": "2o0MPuk8rqCz"
      },
      "source": [
        "En el coraz√≥n de las operaciones de Aerol√≠nea Lucero, Mathias, el gerente de an√°lisis de datos, reuni√≥ a un talentoso equipo de j√≥venes cient√≠ficos de datos para un desaf√≠o crucial: segmentar la base de datos de los clientes. ‚ÄúNuestro objetivo es descubrir patrones en el comportamiento de los pasajeros que nos permitan personalizar servicios y optimizar nuestras campa√±as de marketing,‚Äù explic√≥ Mathias, mientras desplegaba un amplio rango de datos que inclu√≠an desde h√°bitos de compra hasta opiniones sobre los vuelos.\n",
        "\n",
        "Mathias encarg√≥ a los cient√≠ficos de datos la tarea de aplicar t√©cnicas avanzadas de clustering para identificar distintos segmentos de clientes, como los viajeros frecuentes y aquellos que eligen la aerol√≠nea para celebrar ocasiones especiales. La meta principal era entender profundamente c√≥mo estos grupos perciben la calidad y satisfacci√≥n de los servicios ofrecidos por la aerol√≠nea.\n",
        "\n",
        "A trav√©s de un enfoque meticuloso y colaborativo, los cient√≠ficos de datos se abocaron a la tarea, buscando transformar los datos brutos en valiosos insights que permitir√≠an a Aerol√≠nea Lucero no solo mejorar su servicio, sino tambi√©n fortalecer las relaciones con sus clientes mediante una oferta m√°s personalizada y efectiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e78cb41b144041af98928ab26dcfdaa9",
        "deepnote_cell_type": "markdown",
        "id": "hs4KKWF1Hdpo"
      },
      "source": [
        "## Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "95a5533cfd6d49cfb9afc111c44d224f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1714107106552,
        "id": "a4YpMafirqC0",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "acbeab32db6146678e75448dddf43da8",
        "deepnote_cell_type": "markdown",
        "id": "UQOXod4gHhSq"
      },
      "source": [
        "## 1. Estudio de Performance üìà [10 Puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "704b56b978254ad3ae12cdbf58f4832d",
        "deepnote_cell_type": "markdown",
        "id": "Gn5u5ICkrqC2"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/23/b7/6e/23b76e9e77e63c0eec1a7b28372369e3.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d35fbdcc5ef045d6a2822622f0714179",
        "deepnote_cell_type": "markdown",
        "id": "y4Z0jTjtrqC2"
      },
      "source": [
        "Don Mathias les ha encomendado su primera tarea: analizar diversas t√©cnicas de clustering. Su objetivo es entender detalladamente c√≥mo funcionan estos m√©todos en t√©rminos de segmentaci√≥n y eficiencia en tiempo de ejecuci√≥n.\n",
        "\n",
        "Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering (k-means, DBSCAN, Ward y GMM) aplicados a tres conjuntos de datos, incrementando progresivamente su tama√±o. Utilice Plotly para las gr√°ficas y discuta los resultados tanto cualitativa como cuantitativamente.\n",
        "\n",
        "Uno de los requisitos establecidos por Mathias es que el an√°lisis se lleve a cabo utilizando Plotly; de no ser as√≠, se considerar√° incorrecto. Para facilitar este proceso, se ha proporcionado un c√≥digo de Plotly que puede servir como base para realizar las gr√°ficas. Ap√≥yese en el c√≥digo entregado para efectuar el an√°lisis y tome como referencia la siguiente imagen para realizar los gr√°ficos:\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-04-26_at_9.10.44_AM.png' width=800 />\n",
        "\n",
        "En el gr√°fico se visualizan en dos dimensiones los diferentes tipos de datos proporcionados en `datasets`. Cada columna corresponde a un modelo de clustering diferente, mientras que cada fila representa un conjunto de datos distinto. Cada uno de los gr√°ficos incluye el tiempo en segundos que tarda el an√°lisis y la m√©trica Silhouette obtenida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "37580aab6cef4238a8ce42c50a6d35de",
        "deepnote_cell_type": "markdown",
        "id": "maCUNAvZrqC2"
      },
      "source": [
        "Para ser m√°s espec√≠ficos, usted debe cumplir los siguientes objetivos:\n",
        "1. Generar una funci√≥n que permita replicar el gr√°fico expuesto en la imagen (no importa que los colores calcen). [4 puntos]\n",
        "2. Ejecuta la funci√≥n para un `n_samples` igual a 1000, 5000, 10000. [2 puntos]\n",
        "3. Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering utilizando las 3 configuraciones dadas en `n_samples`. [4 puntos]\n",
        "\n",
        "\n",
        "> ‚ùó Tiene libertad absoluta de escoger los hiper par√°metros de los cluster, sin embargo, se recomienda verificar el dominio de las variables para realizar la segmentaci√≥n.\n",
        "\n",
        "> ‚ùó Recuerde que es obligatorio el uso de plotly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "7f7c25e366754595b13fc2e8116f65a0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1714107108441,
        "id": "i0IZPGPOrqC3",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "En la siguiente celda se crean los datos ficticios a usar en la secci√≥n 1 del lab.\n",
        "‚ùóNo realice cambios a esta celda a excepci√≥n de n_samples‚ùó\n",
        "\"\"\"\n",
        "\n",
        "# Datos a utilizar\n",
        "\n",
        "# Configuracion\n",
        "n_samples = 5000 #Este par√°metro si lo pueden modificar\n",
        "\n",
        "def create_data(n_samples):\n",
        "\n",
        "    # Lunas\n",
        "    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)\n",
        "    # Blobs\n",
        "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)\n",
        "    # Datos desiguales\n",
        "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "    mutated = (np.dot(blobs[0], transformation), blobs[1])\n",
        "\n",
        "    # Generamos Dataset\n",
        "    dataset = {\n",
        "        'moons':{\n",
        "            'x': moons[0], 'classes': moons[1], 'n_cluster': 2\n",
        "        },\n",
        "        'blobs':{\n",
        "            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3\n",
        "        },\n",
        "        'mutated':{\n",
        "            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3\n",
        "        }\n",
        "    }\n",
        "    return dataset\n",
        "\n",
        "data_sets = create_data(n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y51s6f_UtIkc"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def evaluate_clustering_performance(data_sets):\n",
        "    \"\"\"\n",
        "    Eval√∫a el desempe√±o de diferentes algoritmos de clustering en varios conjuntos de datos\n",
        "    y genera visualizaciones comparativas usando Plotly.\n",
        "    \"\"\"\n",
        "    # Configuraci√≥n de algoritmos y par√°metros\n",
        "    algorithms_config = {\n",
        "        'K-Means': {\n",
        "            'model': KMeans,\n",
        "            'params': {'n_clusters': None, 'random_state': 42, 'n_init': 10}\n",
        "        },\n",
        "        'DBSCAN': {\n",
        "            'model': DBSCAN,\n",
        "            'params': {'eps': 0.3, 'min_samples': 5}\n",
        "        },\n",
        "        'Ward': {\n",
        "            'model': AgglomerativeClustering,\n",
        "            'params': {'n_clusters': None, 'linkage': 'ward'}\n",
        "        },\n",
        "        'GMM': {\n",
        "            'model': GaussianMixture,\n",
        "            'params': {'n_components': None, 'random_state': 42}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Preparar datos para procesamiento vectorizado\n",
        "    dataset_names = list(data_sets.keys())\n",
        "    algorithm_names = list(algorithms_config.keys())\n",
        "\n",
        "    # Crear figura con subplots\n",
        "    fig = make_subplots(\n",
        "        rows=len(dataset_names),\n",
        "        cols=len(algorithm_names),\n",
        "        subplot_titles=[f\"{algo}<br>{ds}\" for ds in dataset_names for algo in algorithm_names],\n",
        "        horizontal_spacing=0.05,\n",
        "        vertical_spacing=0.2\n",
        "    )\n",
        "\n",
        "    # Procesar cada dataset\n",
        "    for row_idx, (ds_name, ds_data) in enumerate(data_sets.items(), 1):\n",
        "        X = ds_data['x']\n",
        "        n_clusters = ds_data['n_cluster']\n",
        "\n",
        "        # Procesar cada algoritmo\n",
        "        for col_idx, (algo_name, algo_config) in enumerate(algorithms_config.items(), 1):\n",
        "            # Configurar par√°metros espec√≠ficos\n",
        "            params = algo_config['params'].copy()\n",
        "            if algo_name in ['K-Means', 'Ward', 'GMM']:\n",
        "                if 'n_clusters' in params:\n",
        "                    params['n_clusters'] = n_clusters\n",
        "                if 'n_components' in params:\n",
        "                    params['n_components'] = n_clusters\n",
        "\n",
        "            # Medir tiempo de ejecuci√≥n\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # Crear y ajustar modelo\n",
        "                model = algo_config['model'](**params)\n",
        "\n",
        "                # Predecir clusters\n",
        "                if algo_name == 'GMM':\n",
        "                    labels = model.fit_predict(X)\n",
        "                else:\n",
        "                    labels = model.fit_predict(X)\n",
        "\n",
        "                exec_time = time.time() - start_time\n",
        "\n",
        "                # Calcular m√©trica de evaluaci√≥n\n",
        "                unique_labels = np.unique(labels)\n",
        "                if len(unique_labels) > 1:\n",
        "                    sil_score = silhouette_score(X, labels)\n",
        "                else:\n",
        "                    sil_score = 0\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error con {algo_name} en {ds_name}: {str(e)}\")\n",
        "                labels = np.zeros(X.shape[0])\n",
        "                exec_time = 0\n",
        "                sil_score = 0\n",
        "\n",
        "            # Crear visualizaci√≥n\n",
        "            scatter = go.Scatter(\n",
        "                x=X[:, 0], y=X[:, 1],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    color=labels,\n",
        "                    colorscale='viridis',\n",
        "                    showscale=False,\n",
        "                    line=dict(width=0.5, color='gray')\n",
        "                ),\n",
        "                name=f\"{algo_name} - {ds_name}\",\n",
        "                text=f\"Time: {exec_time:.3f}s<br>Silhouette: {sil_score:.3f}\",\n",
        "                hoverinfo='text'\n",
        "            )\n",
        "\n",
        "            fig.add_trace(scatter, row=row_idx, col=col_idx)\n",
        "\n",
        "            # A√±adir informaci√≥n de tiempo y m√©trica\n",
        "            fig.update_xaxes(\n",
        "                title_text=f\"Time: {exec_time:.3f}s\",\n",
        "                row=row_idx,\n",
        "                col=col_idx,\n",
        "                title_font=dict(size=10)\n",
        "            )\n",
        "            fig.update_yaxes(\n",
        "                title_text=f\"Silhouette: {sil_score:.3f}\",\n",
        "                row=row_idx,\n",
        "                col=col_idx,\n",
        "                title_font=dict(size=10)\n",
        "            )\n",
        "\n",
        "    # Configuraci√≥n final del layout\n",
        "    fig.update_layout(\n",
        "        height=900,\n",
        "        width=1200,\n",
        "        showlegend=False,\n",
        "        title_text=\"Comparaci√≥n de Algoritmos de Clustering\"\n",
        "    )\n",
        "\n",
        "    # A√±adir t√≠tulos de filas (nombres de datasets)\n",
        "    for i, ds_name in enumerate(dataset_names, 1):\n",
        "        fig.update_yaxes(title_text=ds_name, row=i, col=1)\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Ejecutar para diferentes tama√±os de muestra\n",
        "n_samples_list = [1000, 5000, 10000]\n",
        "\n",
        "for n_samples in n_samples_list:\n",
        "    print(f\"Evaluando con n_samples = {n_samples}\")\n",
        "    data_sets = create_data(n_samples)\n",
        "    fig = evaluate_clustering_performance(data_sets)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "Il7kcdQKOvEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al realizar la comparaci√≥n de los resultados obtenidos, se puede decir lo siguiente:\n",
        "- Al aumentar el n√∫mero de muestras el tiempo de ejecuci√≥n es mayor\n",
        "- Al aumentar el n√∫mero de muestras Ward comienza a fallar para 'blobs', ya que no se forman clusters definidos\n",
        "- GMM posee el mejor tiempo de ejecuci√≥n con menores muestras, al aumentarlas KMeans tiene mejor tiempo\n",
        "- El que posee peor tiempo es Ward\n",
        "- K-Means y GMM son mejores para blobs, r√°pidos y escalables incluso con 10000 muestras\n",
        "- DBSCAN posee un tiempo que crece pero sigue siendo razonable, al aumentar las muestras se 'fusiona' un cluster"
      ],
      "metadata": {
        "id": "rBUDh6wGmVTz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "13c5cb8067d9415f83b3d497954a437a",
        "deepnote_cell_type": "markdown",
        "id": "3mCbZc86rqC6"
      },
      "source": [
        "## 2. An√°lisis de Satisfacci√≥n de Vuelos. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd6e991646b44f50a4b13f01d1542415",
        "deepnote_cell_type": "markdown",
        "id": "JI33m5jbrqC6"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media4.giphy.com/media/v1.Y2lkPTZjMDliOTUyb3B5Y3BtbTZwMnB0ZXRyejFpanJkNDl5cGhoeWlsc2k5bGx1MTUwYSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/l4FARHkIFJReGSy2c/giphy.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5742dfbd5a2e43778ff250436bab1005",
        "deepnote_cell_type": "markdown",
        "id": "h5k24znirqC7"
      },
      "source": [
        "Habiendo entendido c√≥mo funcionan los modelos de aprendizaje no supervisado, *Don Mathias* le encomienda estudiar la satisfacci√≥n de pasajeros al haber tomado un vuelo en alguna de sus aerolineas. Para esto, el magnate le dispone del dataset `aerolineas_licer.parquet`, el cual contiene el grado de satisfacci√≥n de los clientes frente a diferentes aspectos del vuelo. Las caracter√≠sticas del vuelo se definen a continuaci√≥n:\n",
        "\n",
        "- *Gender*: G√©nero de los pasajeros (Femenino, Masculino)\n",
        "- *Customer Type*: Tipo de cliente (Cliente habitual, cliente no habitual)\n",
        "- *Age*: Edad actual de los pasajeros\n",
        "- *Type of Travel*: Prop√≥sito del vuelo de los pasajeros (Viaje personal, Viaje de negocios)\n",
        "- *Class*: Clase de viaje en el avi√≥n de los pasajeros (Business, Eco, Eco Plus)\n",
        "- *Flight distance*: Distancia del vuelo de este viaje\n",
        "- *Inflight wifi service*: Nivel de satisfacci√≥n del servicio de wifi durante el vuelo (0:No Aplicable; 1-5)\n",
        "- *Departure/Arrival time convenient*: Nivel de satisfacci√≥n con la conveniencia del horario de salida/llegada\n",
        "- *Ease of Online booking*: Nivel de satisfacci√≥n con la facilidad de reserva en l√≠nea\n",
        "- *Gate location*: Nivel de satisfacci√≥n con la ubicaci√≥n de la puerta\n",
        "- *Food and drink*: Nivel de satisfacci√≥n con la comida y la bebida\n",
        "- *Online boarding*: Nivel de satisfacci√≥n con el embarque en l√≠nea\n",
        "- *Seat comfort*: Nivel de satisfacci√≥n con la comodidad del asiento\n",
        "- *Inflight entertainment*: Nivel de satisfacci√≥n con el entretenimiento durante el vuelo\n",
        "- *On-board service*: Nivel de satisfacci√≥n con el servicio a bordo\n",
        "- *Leg room service*: Nivel de satisfacci√≥n con el espacio para las piernas\n",
        "- *Baggage handling*: Nivel de satisfacci√≥n con el manejo del equipaje\n",
        "- *Check-in service*: Nivel de satisfacci√≥n con el servicio de check-in\n",
        "- *Inflight service*: Nivel de satisfacci√≥n con el servicio durante el vuelo\n",
        "- *Cleanliness*: Nivel de satisfacci√≥n con la limpieza\n",
        "- *Departure Delay in Minutes*: Minutos de retraso en la salida\n",
        "- *Arrival Delay in Minutes*: Minutos de retraso en la llegada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoIFHpw5xCW"
      },
      "source": [
        "En consideraci√≥n de lo anterior, realice las siguientes tareas:\n",
        "\n",
        "0. Ingeste el dataset a su ambiente de trabajo.\n",
        "\n",
        "1. Seleccione **s√≥lo las variables num√©ricas del dataset**.  Explique qu√© √©fectos podr√≠a causar el uso de variables categ√≥ricas en un algoritmo no supervisado. [2 punto]\n",
        "\n",
        "2. Realice una visualizaci√≥n de la distribuci√≥n de cada variable y analice cada una de estas distribuciones. [2 punto]\n",
        "\n",
        "3. Bas√°ndose en los gr√°ficos, eval√∫e la necesidad de escalar los datos y explique el motivo de su decisi√≥n. [2 puntos]\n",
        "\n",
        "4. Examine la correlaci√≥n entre las variables mediante un correlograma. [2 puntos]\n",
        "\n",
        "5. De acuerdo con los resultados obtenidos en 4, reduzca la dimensionalidad del conjunto de datos a cuatro variables, justificando su elecci√≥n respecto a las variables que decide eliminar. [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6tcVBCtxxS"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzHTZ17xveU_"
      },
      "outputs": [],
      "source": [
        "# 0: Se ingresa el dataset al ambiente de trabajo\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Cargar el dataset\n",
        "path = '/content/drive/MyDrive/MDS7202/aerolineas_lucer.parquet'\n",
        "df = pd.read_parquet(path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1: Seleccionar solo las variables numericas\n",
        "df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
        "df_numeric.head()"
      ],
      "metadata": {
        "id": "52ig0yfrJBu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicacion: El uso de variables categ√≥ricas en un algoritmo no supervisado puede ser problem√°tico porque muchos algoritmos como K-means o PCA, requieren que las variables sean num√©ricas. Las variables categ√≥ricas no pueden ser interpretadas directamente en estos algoritmos, por lo que deben ser codificadas primero."
      ],
      "metadata": {
        "id": "ObL5zPwQJyWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Visualizacion de cada una de las variables numericas\n",
        "import plotly.express as px\n",
        "\n",
        "for col in df_numeric.columns:\n",
        "    fig = px.histogram(df_numeric, x=col, nbins=30, title=f'Distribuci√≥n de variable {col}')\n",
        "    fig.update_layout(\n",
        "        xaxis_title=col,\n",
        "        yaxis_title='Frecuencia',\n",
        "        bargap=0.2\n",
        "    )\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "ynOSKxiHMeci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta**\n",
        "\n",
        "1. **ID**  \n",
        "   La variable `id` presenta una distribuci√≥n uniforme, lo que confirma que se trata √∫nicamente de un identificador y no tiene valor anal√≠tico para los modelos.\n",
        "\n",
        "2. **Age**  \n",
        "   La edad de los pasajeros muestra una distribuci√≥n unimodal con forma aproximadamente normal, concentrada entre los 20 y 50 a√±os. Los adultos j√≥venes y de mediana edad representan la mayor parte de la muestra, mientras que los pasajeros muy j√≥venes (<15) o mayores (>70) tienen una presencia marginal.\n",
        "\n",
        "3. **Flight Distance**  \n",
        "   La variable de distancia de vuelo presenta una distribuci√≥n asim√©trica hacia la derecha, con la mayor√≠a de los viajes siendo de corta o media distancia (200‚Äì1000 km). Los vuelos de larga distancia son menos frecuentes, lo cual es consistente con el patr√≥n de tr√°fico a√©reo mayormente regional o dom√©stico.\n",
        "\n",
        "4. **Departure Delay y Arrival Delay**  \n",
        "   Ambas variables presentan distribuciones altamente sesgadas a la derecha, reflejando que la mayor√≠a de los vuelos tienen retrasos m√≠nimos o nulos. Sin embargo, existen valores extremos poco frecuentes que corresponden a vuelos con retrasos significativos.\n",
        "\n",
        "5. **Niveles de satisfacci√≥n**  \n",
        "   Las variables de satisfacci√≥n relacionadas con los diferentes aspectos del servicio (wifi, comida, entretenimiento, limpieza, etc.) muestran distribuciones relativamente aleatorias, aunque con mayor frecuencia entre valores intermedios y altos (3 a 5). Esto sugiere que, en general, los pasajeros eval√∫an su experiencia de vuelo como aceptable a buena, con menor incidencia de puntuaciones bajas.\n"
      ],
      "metadata": {
        "id": "zOpQQ7DK4CuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Escalado de datos\n",
        "# Analizando la distribucion de los datos se observa que los datos tienen frecuencias muy variadas, ...\n",
        "# ... por lo que se escoge la opcion de escalar los datos\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_normalized = pd.DataFrame(scaler.fit_transform(df_numeric), columns=df_numeric.columns)\n",
        "df_normalized.head()"
      ],
      "metadata": {
        "id": "LgGQPiZQOG-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Analisis de correlacion\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Correlacion entre las variables\n",
        "corr_matrix = df_normalized.corr()\n",
        "\n",
        "# Crear heatmap de correlaciones\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(corr_matrix,\n",
        "            annot=True,\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            fmt='.2f',\n",
        "            square=True)\n",
        "plt.title('Matriz de Correlaci√≥n - Variables Num√©ricas')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AcK50D-VP8-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Analisis de correlacion\n",
        "\n",
        "# Se calcula la correlacion promedio absoluta\n",
        "corr_abs_matrix = corr_matrix.abs()\n",
        "\n",
        "# Excluir la diagonal\n",
        "np.fill_diagonal(corr_abs_matrix.values, np.nan)\n",
        "\n",
        "# Calcular promedio de correlacion para cada variable\n",
        "average_correlation = corr_abs_matrix.mean(axis=0, skipna=True)\n",
        "\n",
        "# Crear DataFrame ordenado de menor a mayor correlacion promedio\n",
        "correlation_ranking = pd.DataFrame({\n",
        "    'Variable': average_correlation.index,\n",
        "    'Correlacion_Promedio_Absoluta': average_correlation.values\n",
        "}).sort_values('Correlacion_Promedio_Absoluta', ascending=True)\n",
        "\n",
        "# Mostrar top de variables con menor correlacion promedio\n",
        "print(\"=\" * 50)\n",
        "print(\"TOP 10 VARIABLES CON MENOR CORRELACI√ìN PROMEDIO\")\n",
        "print(\"=\" * 50)\n",
        "for i, (idx, row) in enumerate(correlation_ranking.head(15).iterrows()):\n",
        "    print(f\"{i+1:2d}. {row['Variable']:25s}: {row['Correlacion_Promedio_Absoluta']:.4f}\")"
      ],
      "metadata": {
        "id": "cxRzmupnLTBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Se escogen 4 variables con baja correlacion y representativas\n",
        "selected_vars = [\n",
        "    'Age',                                                   # Representa demografia\n",
        "    'Arrival Delay in Minutes',            # Representa demoras en el vuelo\n",
        "    'Flight Distance',                             # Representa caracteristicas en el vuelo\n",
        "    'Checkin service'                             # Respresenta calidad del servicio\n",
        "]\n",
        "\n",
        "# Dataset reducido\n",
        "df_reduced = df_normalized[selected_vars].copy()\n",
        "df_reduced.head()"
      ],
      "metadata": {
        "id": "HE6ZzMEUTj2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4b6c047d994f40ea9e78a36a777042e0",
        "deepnote_cell_type": "markdown",
        "id": "PNGfTgtkrqC9"
      },
      "source": [
        "## 3. Preprocesamiento üé≠. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "713b3f0e61dd4841bb5b38c730d344d5",
        "deepnote_cell_type": "markdown",
        "id": "6RZD0fMNrqC-"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media.tenor.com/R_WseIIwQ8QAAAAM/beavis-computer.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "98400c7b5fec4af193eec3601f53891e",
        "deepnote_cell_type": "markdown",
        "id": "J6d4VEOTrqC-"
      },
      "source": [
        "Tras quedar satisfecho con los resultados presentados en el punto 2, el due√±o de la empresa ha solicitado que se preprocesen los datos mediante un `pipeline`. Es crucial que este proceso tenga en cuenta las observaciones derivadas de los an√°lisis anteriores. Adicionalmente, ha expresado su inter√©s en visualizar el conjunto de datos en un gr√°fico de dos o tres dimensiones.\n",
        "\n",
        "Bas√°ndose en los an√°lisis realizados anteriormente:\n",
        "1. Cree un `pipeline` que incluya PCA, utilizando las consideraciones mencionadas previamente para proyectar los datos a dos dimensiones. [4 puntos]\n",
        "2. Grafique los resultados obtenidos y comente lo visualizado. [6 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDSaGoq0OUp"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ad1e70818ad748638ca0927b07a76125",
        "deepnote_cell_type": "code",
        "id": "gBYG238wrqC-"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Crear el pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),          # Escalado de los datos\n",
        "    ('pca', PCA(n_components=2))   # Reducci√≥n a 2 dimensiones\n",
        "])\n",
        "\n",
        "# Ajustar el pipeline a los datos\n",
        "df_scaled = pipeline.fit_transform(df_reduced)\n",
        "\n",
        "# Convertir el resultado de PCA en un DataFrame\n",
        "df_pca = pd.DataFrame(df_scaled, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Grafico\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(x='PC1', y='PC2', data=df_pca,  alpha=0.7)\n",
        "plt.title('PCA: Proyecci√≥n de los datos a 2 dimensiones')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta**\n",
        "\n",
        "La proyecci√≥n en dos dimensiones mediante PCA muestra que los datos se concentran principalmente en una nube densa en la parte inferior del gr√°fico, cercana al eje horizontal, con valores de la Componente Principal 2 pr√≥ximos a cero y extendi√©ndose hacia valores m√°s altos en menor densidad. Esto indica que la mayor parte de la varianza de los datos se concentra en un rango acotado, mientras que los puntos que aparecen m√°s alejados verticalmente corresponden a casos menos frecuentes o posibles outliers que presentan caracter√≠sticas at√≠picas respecto a la mayor√≠a."
      ],
      "metadata": {
        "id": "-MywTZwf4k9K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bd281470d3054764a63d857cfa7d52a6",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "7ENoOtIIrqC_"
      },
      "source": [
        "## 4. Outliers üö´üôÖ‚Äç‚ôÄÔ∏è‚ùåüôÖ‚Äç‚ôÇÔ∏è [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db89e9c9f35c44abbd8991180226c0ea",
        "deepnote_cell_type": "markdown",
        "id": "fbGw6Sa-rqC_"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://joachim-gassen.github.io/images/ani_sim_bad_leverage.gif\" width=250>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3e2f59fa12954641af7a854a4e203694",
        "deepnote_cell_type": "markdown",
        "id": "nl_ccu9brqDA"
      },
      "source": [
        "Con el objetivo de mantener la claridad en su an√°lisis, Don Mathias le ha solicitado entrenar un modelo que identifique pasajeros con comportamientos altamente at√≠picos.\n",
        "\n",
        "1. Utilice `IsolationForest` para clasificar las anomal√≠as del dataset (sin aplicar PCA), configurando el modelo para que s√≥lo el 1% de los datos sean considerados an√≥malos. Aseg√∫rese de integrar esta tarea dentro de un `pipeline`. [3 puntos]\n",
        "\n",
        "2. Visualice los resultados en el gr√°fico de dos dimensiones previamente creado. [3 puntos]\n",
        "\n",
        "3. ¬øC√≥mo evaluar√≠a el rendimiento de su modelo en la detecci√≥n de anomal√≠as? [4 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cS1FR00NlF"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "be86896911244aa89e3b5f3f00a286af",
        "deepnote_cell_type": "code",
        "id": "iaPZFmjyrqDA"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Se crea el pipeline con escalado y Isolation Forest\n",
        "outlier_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),                   # Escalar los datos\n",
        "    ('isolation_forest', IsolationForest(\n",
        "        contamination=0.01,                          # 1% de outliers\n",
        "        random_state=7202,\n",
        "        n_estimators=100\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Se entrena el modelo y se predicen los outliers\n",
        "outlier_pipeline.fit(df_reduced)\n",
        "outlier_predictions = outlier_pipeline.predict(df_reduced)\n",
        "outlier_labels = np.where(outlier_predictions == 1, 'Normal', 'Outlier')\n",
        "\n",
        "# Se agregan las etiquetas al dataframe original\n",
        "df_reduced['outlier_label'] = outlier_labels\n",
        "df['outlier_label'] = outlier_labels\n",
        "\n",
        "# Contar la cantidad de outliers detectados\n",
        "outlier_count = sum(outlier_predictions == -1)\n",
        "total_samples = len(outlier_predictions)\n",
        "\n",
        "print(f\"Total de muestras: {total_samples}\")\n",
        "print(f\"Outliers detectados (1%): {outlier_count}\")\n",
        "print(f\"Porcentaje real: {(outlier_count/total_samples)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se copia el dataframe\n",
        "df_reduced_for_pca = df_reduced.copy()\n",
        "df_reduced_for_pca['outlier_label'] = outlier_labels\n",
        "\n",
        "# Se aplica PCA\n",
        "pca_vis = PCA(n_components=2)\n",
        "pca_result = pca_vis.fit_transform(StandardScaler().fit_transform(df_reduced_for_pca.drop('outlier_label', axis=1)))\n",
        "\n",
        "# Se crea un dataframe para la visualizacion\n",
        "df_outliers = pd.DataFrame({\n",
        "    'PC1': pca_result[:, 0],\n",
        "    'PC2': pca_result[:, 1],\n",
        "    'outlier_label': df_reduced_for_pca['outlier_label'],\n",
        "    'is_outlier': outlier_predictions == -1\n",
        "})\n",
        "\n",
        "# Grafico\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Puntos normales\n",
        "normal_points = df_outliers[df_outliers['outlier_label'] == 'Normal']\n",
        "plt.scatter(normal_points['PC1'], normal_points['PC2'],\n",
        "           alpha=0.6, c='blue', label='Normal', s=30)\n",
        "\n",
        "# Puntos outliers\n",
        "outlier_points = df_outliers[df_outliers['outlier_label'] == 'Outlier']\n",
        "plt.scatter(outlier_points['PC1'], outlier_points['PC2'],\n",
        "           alpha=0.8, c='red', label='Outlier', s=50, edgecolors='black')\n",
        "\n",
        "plt.xlabel('Primera Componente Principal (PC1)')\n",
        "plt.ylabel('Segunda Componente Principal (PC2)')\n",
        "plt.title('Detecci√≥n de Outliers usando Isolation Forest\\n(1% de los datos como an√≥malos)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I5YsfMFzduoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear dataframe sin outliers\n",
        "df_sin_outliers = df_reduced[outlier_predictions != -1].copy()\n",
        "\n",
        "# Aplicar PCA al dataframe sin outliers, excluyendo la columna 'outlier_label'\n",
        "pca_sin_outliers = PCA(n_components=2)\n",
        "pca_result_sin_outliers = pca_sin_outliers.fit_transform(StandardScaler().fit_transform(df_sin_outliers.drop('outlier_label', axis=1)))\n",
        "\n",
        "# Graficar las componentes principales sin outliers\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(pca_result_sin_outliers[:, 0], pca_result_sin_outliers[:, 1],\n",
        "           alpha=0.6, c='blue', s=30)\n",
        "plt.xlabel(f'PC1 ({pca_sin_outliers.explained_variance_ratio_[0]:.2%} varianza)')\n",
        "plt.ylabel(f'PC2 ({pca_sin_outliers.explained_variance_ratio_[1]:.2%} varianza)')\n",
        "plt.title('Componentes Principales - Datos sin Outliers')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qM3WwXOgt9ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuestas:**\n",
        "\n",
        "Los resultados visualizados en el espacio reducido con PCA muestran que la mayor√≠a de los puntos clasificados como an√≥malos se encuentran en la periferia de la nube de datos, especialmente en zonas de baja densidad o con valores extremos en la segunda componente principal. Esto es consistente con el comportamiento esperado de un modelo de detecci√≥n de anomal√≠as, ya que se√±ala aquellos pasajeros cuyos patrones difieren significativamente de la mayor√≠a."
      ],
      "metadata": {
        "id": "6IgKCNWgWCy9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3871e2fe5bdd422dbdbfaebf75503ae3",
        "deepnote_cell_type": "markdown",
        "id": "zQFTklmVrqDB"
      },
      "source": [
        "## 5. M√©tricas de Desempe√±o üöÄ [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "236333de6dd445c182aefcc507589325",
        "deepnote_cell_type": "markdown",
        "id": "YpNj4wbPrqDB"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.imgflip.com/6xz0ij.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a7e1ceb91be94b1da2ab8be97dfac999",
        "deepnote_cell_type": "markdown",
        "id": "CR3hzRxrrqDB"
      },
      "source": [
        "Motivado por incrementar su fortuna, Don Mathias le solicita entrenar un modelo que le permita segmentar a los pasajeros en grupos distintos, con el objetivo de optimizar las diversas campa√±as de marketing dise√±adas por su equipo. Para ello, le se pide realizar las siguientes tareas:\n",
        "\n",
        "1. Utilizar el modelo **Gaussian Mixture** y explore diferentes configuraciones de n√∫mero de cl√∫sters, espec√≠ficamente entre 3 y 8. Aseg√∫rese de integrar esta operaci√≥n dentro de un `pipeline`. [4 puntos]\n",
        "2. Explique cu√°l ser√≠a el criterio adecuado para seleccionar el n√∫mero √≥ptimo de cl√∫sters. **Justifique de forma estadistica y a traves de gr√°ficos.** [6 puntos]\n",
        "\n",
        "> **HINT:** Se recomienda investigar sobre los criterios AIC y BIC para esta tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_T_zTg0MXB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "6d3d1bb3fda14321984466d9101a775a",
        "deepnote_cell_type": "code",
        "id": "5GeUb9J3rqDB"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Se crea el pipeline\n",
        "gmm_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gmm', GaussianMixture(random_state=7202))\n",
        "])\n",
        "\n",
        "# Se define el rango de clusters a probar\n",
        "n_components_range = range(3, 9)\n",
        "results = {}\n",
        "\n",
        "for n_components in n_components_range:\n",
        "    print(f\"Entrenando modelo con {n_components} clusters...\")\n",
        "\n",
        "    # Se configura y entrena el modelo\n",
        "    gmm_pipeline.set_params(gmm__n_components=n_components)\n",
        "    gmm_pipeline.fit(df_outliers.drop('outlier_label', axis=1))\n",
        "\n",
        "    # Se almacenan los resultados\n",
        "    results[n_components] = {\n",
        "        'model': gmm_pipeline.named_steps['gmm'],\n",
        "        'bic': gmm_pipeline.named_steps['gmm'].bic(StandardScaler().fit_transform(df_outliers.drop('outlier_label', axis=1))),\n",
        "        'aic': gmm_pipeline.named_steps['gmm'].aic(StandardScaler().fit_transform(df_outliers.drop('outlier_label', axis=1))),\n",
        "        'converged': gmm_pipeline.named_steps['gmm'].converged_,\n",
        "        'n_iter': gmm_pipeline.named_steps['gmm'].n_iter_\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se extraen las metricas de AIC y BIC\n",
        "aic_scores = [results[n]['aic'] for n in n_components_range]\n",
        "bic_scores = [results[n]['bic'] for n in n_components_range]\n",
        "\n",
        "# Curvas de AIC y BIC\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(n_components_range, bic_scores, 'bo-', linewidth=2, markersize=8, label='BIC')\n",
        "plt.plot(n_components_range, aic_scores, 'ro-', linewidth=2, markersize=8, label='AIC')\n",
        "plt.xlabel('N√∫mero de Clusters')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Criterios AIC y BIC para Selecci√≥n de N√∫mero de Clusters')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(n_components_range)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calcular diferencias y porcentajes de mejora\n",
        "aic_differences = np.diff(aic_scores)\n",
        "bic_differences = np.diff(bic_scores)\n",
        "\n",
        "# Encontrar el punto de codo (donde la mejora se estabiliza)\n",
        "optimal_aic = n_components_range[np.argmin(aic_scores)]\n",
        "optimal_bic = n_components_range[np.argmin(bic_scores)]\n",
        "\n",
        "print(\"=\" * 45)\n",
        "print(\"RECOMENDACI√ìN DE N√öMERO √ìPTIMO DE CLUSTERS\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"Seg√∫n BIC: {optimal_bic} clusters\")\n",
        "print(f\"Seg√∫n AIC: {optimal_aic} clusters\")"
      ],
      "metadata": {
        "id": "gnh2E9LHaVHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criterio del codo\n",
        "# Se busca el punto donde la mejora marginal es menor\n",
        "aic_improvement_ratio = np.abs(aic_differences / aic_scores[:-1]) * 100\n",
        "bic_improvement_ratio = np.abs(bic_differences / bic_scores[:-1]) * 100\n",
        "\n",
        "print(f\"M√≠nimo AIC encontrado: {min(aic_scores):.2f} con {optimal_aic} clusters\")\n",
        "print(f\"M√≠nimo BIC encontrado: {min(bic_scores):.2f} con {optimal_bic} clusters\")\n",
        "\n",
        "# Grafico de mejora marginal\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(4, 9), aic_improvement_ratio, 'ro-', label='AIC % Improvement')\n",
        "plt.plot(range(4, 9), bic_improvement_ratio, 'bo-', label='BIC % Improvement')\n",
        "plt.xlabel('N√∫mero de Clusters')\n",
        "plt.ylabel('Porcentaje de Mejora (%)')\n",
        "plt.title('Mejora Marginal al A√±adir Clusters Adicionales')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(range(4, 9))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Decision Final\n",
        "if optimal_bic == optimal_aic:\n",
        "    print(f\"N√∫mero √≥ptimo de clusters: {optimal_bic} (consenso entre AIC y BIC)\")\n",
        "elif optimal_bic < optimal_aic:\n",
        "    print(f\"N√∫mero √≥ptimo de clusters: {optimal_bic} (seg√∫n BIC)\")\n",
        "else:\n",
        "    print(f\"N√∫mero √≥ptimo de clusters: {optimal_aic} (seg√∫n AIC)\")\n",
        "\n",
        "# Entrenar modelo final con el numero optimo\n",
        "optimal_n = optimal_bic\n",
        "final_gmm_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gmm', GaussianMixture(n_components=optimal_n, random_state=7202))\n",
        "])\n",
        "\n",
        "# Excluir la columna 'outlier_label' antes de entrenar y predecir\n",
        "final_gmm_pipeline.fit(df_outliers.drop('outlier_label', axis=1))\n",
        "cluster_labels = final_gmm_pipeline.predict(df_outliers.drop('outlier_label', axis=1))\n",
        "\n",
        "print(f\"\\nModelo final entrenado con {optimal_n} clusters\")\n",
        "print(f\"Clusters asignados: {np.unique(cluster_labels)}\")\n",
        "print(f\"Tama√±o de cada cluster: {np.bincount(cluster_labels)}\")"
      ],
      "metadata": {
        "id": "FOcENs29aW-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuestas:**\n",
        "Los criterios AIC y BIC son adecuados para determinar la cantidad √≥ptima de cl√∫sters en un modelo de mezclas gaussianas porque balancean dos aspectos fundamentales: el ajuste del modelo a los datos y la complejidad del mismo. Ambos criterios parten del principio de m√°xima verosimilitud, premiando aquellos modelos que mejor explican los datos, pero penalizando el exceso de par√°metros que podr√≠a llevar a sobreajuste.\n",
        "\n",
        "En base a los resultados obtenidos, se observa un gran aumento en la mejora marginal al pasar de 4 a 5 cl√∫sters, lo que indica que el modelo captura de manera m√°s efectiva la estructura de los datos en ese punto. Sin embargo, al seguir aumentando la cantidad de clusters se mantiene un incremento adicional en la precisi√≥n del modelo, aunque m√°s moderado, lo que sugiere que este ajuste sigue aportando valor sin caer en sobreajuste excesivo."
      ],
      "metadata": {
        "id": "i1qvfIzgocBa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dd342e336254418ba766b29dce16b267",
        "deepnote_cell_type": "markdown",
        "id": "P9CERnaerqDC"
      },
      "source": [
        "## 6. An√°lisis de resultados üìä [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "953b5ad01a704b50b899db7176d1b7b2",
        "deepnote_cell_type": "markdown",
        "id": "I1yNa111rqDC"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5b/03/4e/5b034e96d84c6c6b57a9a04ca14aac02.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd90e2f135404353ac0b5ab844936ca7",
        "deepnote_cell_type": "markdown",
        "id": "dg0Qx4RZrqDC"
      },
      "source": [
        "Una vez identificado el n√∫mero √≥ptimo de cl√∫sters, se le pide realizar lo siguiente:\n",
        "\n",
        "1. Utilizar la proyecci√≥n en dos dimensiones para visualizar cada cl√∫ster claramente. [2 puntos]\n",
        "\n",
        "2. ¬øEs posible distinguir claramente entre los cl√∫sters generados? [2 puntos]\n",
        "\n",
        "3. Proporcionar una descripci√≥n breve de cada cl√∫ster utilizando estad√≠sticas descriptivas b√°sicas, como la media y la desviaci√≥n est√°ndar, para resumir las caracter√≠sticas de las variables utilizadas en estos algoritmos. [2 puntos]\n",
        "\n",
        "4. Proceda a visualizar los cl√∫sters en tres dimensiones para una perspectiva m√°s detallada. [2 puntos]\n",
        "\n",
        "5. ¬øC√≥mo afecta esto a sus conclusiones anteriores? [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRN0zZip0IMB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Proyeccion de dos dimensiones\n",
        "# Crear dataframe sin outliers\n",
        "df_sin_outliers = df_reduced[outlier_predictions != -1].copy()\n",
        "\n",
        "# Aplicar PCA para 2D visualizaci√≥n (una vez para todos los modelos)\n",
        "pca_sin_outliers_vis = PCA(n_components=2)\n",
        "X_pca_sin_outliers_2d = pca_sin_outliers_vis.fit_transform(\n",
        "    StandardScaler().fit_transform(df_sin_outliers.drop('outlier_label', axis=1))\n",
        ")\n",
        "\n",
        "# Configurar subplots para 6 modelos (3 a 8 clusters)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Rango de clusters a evaluar\n",
        "n_clusters_range = range(3, 9)\n",
        "\n",
        "# Diccionario para almacenar resultados\n",
        "cluster_results = {}\n",
        "\n",
        "for i, n_clusters in enumerate(n_clusters_range):\n",
        "    # Entrenar modelo Gaussian Mixture\n",
        "    gmm = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('gmm', GaussianMixture(n_components=n_clusters, random_state=42))\n",
        "    ])\n",
        "\n",
        "    gmm.fit(df_sin_outliers.drop('outlier_label', axis=1))\n",
        "    cluster_labels = gmm.predict(df_sin_outliers.drop('outlier_label', axis=1))\n",
        "\n",
        "    # Almacenar resultados\n",
        "    cluster_results[n_clusters] = {\n",
        "        'labels': cluster_labels,\n",
        "        'model': gmm,\n",
        "        'sizes': np.bincount(cluster_labels)\n",
        "    }\n",
        "\n",
        "    # Graficar\n",
        "    scatter = axes[i].scatter(X_pca_sin_outliers_2d[:, 0], X_pca_sin_outliers_2d[:, 1],\n",
        "                            c=cluster_labels,\n",
        "                            cmap='tab10',\n",
        "                            alpha=0.7,\n",
        "                            s=40,\n",
        "                            edgecolors='w',\n",
        "                            linewidth=0.3,\n",
        "                            vmin=0,\n",
        "                            vmax=n_clusters-1)\n",
        "\n",
        "    axes[i].set_xlabel(f'PC1 ({pca_sin_outliers_vis.explained_variance_ratio_[0]:.2%})')\n",
        "    axes[i].set_ylabel(f'PC2 ({pca_sin_outliers_vis.explained_variance_ratio_[1]:.2%})')\n",
        "    axes[i].set_title(f'{n_clusters} Clusters')\n",
        "    axes[i].grid(True, alpha=0.2)\n",
        "\n",
        "# Ajustar layout\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Comparaci√≥n de Modelos Gaussian Mixture con Diferentes N√∫meros de Clusters',\n",
        "             fontsize=16, y=1.02)\n",
        "plt.show()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cRAvtOxjvvLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  2. Analisis de los clusters\n",
        "**Respuesta**:\n",
        "\n",
        "Al observar los gr√°ficos generados para diferentes n√∫meros de cl√∫sters se aprecia que en general los grupos presentan un nivel importante de solapamiento, lo que dificulta distinguir fronteras claras entre ellos en la proyecci√≥n 2D obtenida con PCA. A medida que aumenta la cantidad de cl√∫sters, se generan divisiones m√°s peque√±as y detalladas, pero tambi√©n se incrementa la superposici√≥n, lo que complica la interpretaci√≥n visual de los resultados. En consecuencia, si bien los modelos con m√°s cl√∫sters pueden capturar mayor heterogeneidad estad√≠stica en los datos, no necesariamente se traducen en una separaci√≥n clara y f√°cilmente interpretable de los grupos en el espacio reducido.\n",
        "\n",
        "En este contexto, se decide trabajar a partir de este punto con el modelo de 4 cl√∫sters, ya que este proporciona la mejor diferenciaci√≥n observable entre los grupos, aun cuando estad√≠sticamente no sea el modelo √≥ptimo seg√∫n los criterios AIC y BIC.\n"
      ],
      "metadata": {
        "id": "paSXizmPxw4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Analisis Estadistico de los clusters\n",
        "\n",
        "# Asumimos que ya tenemos el modelo de 4 clusters entrenado\n",
        "n_clusters = 4\n",
        "gmm_4 = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gmm', GaussianMixture(n_components=n_clusters, random_state=42))\n",
        "])\n",
        "\n",
        "df_sin_outliers = df_reduced[outlier_predictions != -1].copy()\n",
        "gmm_4.fit(df_sin_outliers.drop('outlier_label', axis=1))\n",
        "cluster_labels_4 = gmm_4.predict(df_sin_outliers.drop('outlier_label', axis=1))\n",
        "\n",
        "# A√±adir etiquetas de cluster al dataframe\n",
        "df_clustered = df_sin_outliers.drop('outlier_label', axis=1).copy()\n",
        "df_clustered['Cluster'] = cluster_labels_4\n",
        "\n",
        "# Estadisticas descriptivas por cluster\n",
        "print(\"=\" * 80)\n",
        "print(\"DESCRIPCI√ìN ESTAD√çSTICA DE LOS 4 CLUSTERS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Tama√±o de cada cluster\n",
        "cluster_sizes = np.bincount(cluster_labels_4)\n",
        "cluster_percentages = (cluster_sizes / len(cluster_labels_4)) * 100\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    print(f\"\\nCLUSTER {cluster_id} - {cluster_sizes[cluster_id]} muestras ({cluster_percentages[cluster_id]:.1f}%)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
        "\n",
        "    # Estadisticas b√°sicas\n",
        "    print(\"Estad√≠sticas descriptivas:\")\n",
        "    for column in df_clustered.columns[:-1]:\n",
        "        mean_val = cluster_data[column].mean()\n",
        "        std_val = cluster_data[column].std()\n",
        "        print(f\"  {column:25s}: {mean_val:.2f} ¬± {std_val:.2f}\")\n",
        "\n",
        "# Comparacion con la media global\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARACI√ìN CON LA MEDIA GLOBAL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "global_means = df_clustered.drop('Cluster', axis=1).mean()\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
        "    cluster_means = cluster_data.mean()\n",
        "\n",
        "    print(f\"\\nCLUSTER {cluster_id} - Diferencias respecto a la media global:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    differences = (cluster_means.drop('Cluster') - global_means).abs().sort_values(ascending=False)\n",
        "\n",
        "    for var in differences.index[:5]:\n",
        "        cluster_mean = cluster_means[var]\n",
        "        global_mean = global_means[var]\n",
        "        diff = cluster_mean - global_mean\n",
        "        diff_pct = (diff / global_mean) * 100 if global_mean != 0 else 0\n",
        "\n",
        "        print(f\"  {var:25s}: {cluster_mean:.2f} vs {global_mean:.2f} \"\n",
        "              f\"(Œî: {diff:+.2f}, {diff_pct:+.1f}%)\")\n",
        "\n",
        "# Heatmap de medias por cluster\n",
        "plt.figure(figsize=(14, 8))\n",
        "cluster_means = df_clustered.groupby('Cluster').mean()\n",
        "\n",
        "# Normalizar por columna para mejor visualizacion\n",
        "normalized_means = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n",
        "\n",
        "sns.heatmap(normalized_means.T,\n",
        "            annot=cluster_means.T.round(2),\n",
        "            cmap='RdYlBu_r',\n",
        "            fmt='.2f',\n",
        "            cbar_kws={'label': 'Valor normalizado'},\n",
        "            annot_kws={'size': 10})\n",
        "plt.title('Valores Promedio por Cluster (Valores reales en anotaciones)')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Variable')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fG8Z1jkG1cSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Visualizacion de los cl√∫sters en tres dimensiones\n",
        "\n",
        "df_sin_outliers = df_reduced[outlier_predictions != -1].copy()\n",
        "\n",
        "# Se aplica PCA de 3 dimensiones\n",
        "pca_3d = PCA(n_components=3)\n",
        "X_pca_3d = pca_3d.fit_transform(StandardScaler().fit_transform(df_sin_outliers.drop('outlier_label', axis=1)))\n",
        "\n",
        "# Se crea nuevo dataframe con las componentes PCA\n",
        "df_pca_3d = pd.DataFrame({\n",
        "    'PC1': X_pca_3d[:, 0],\n",
        "    'PC2': X_pca_3d[:, 1],\n",
        "    'PC3': X_pca_3d[:, 2]\n",
        "})\n",
        "\n",
        "gmm_3d = GaussianMixture(n_components=4, random_state=42)\n",
        "cluster_labels_3d = gmm_3d.fit_predict(df_pca_3d)\n",
        "\n",
        "df_pca_3d['Cluster'] = cluster_labels_3d\n",
        "\n",
        "# Visualizacion 3D\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(df_pca_3d['PC1'],\n",
        "                    df_pca_3d['PC2'],\n",
        "                    df_pca_3d['PC3'],\n",
        "                    c=df_pca_3d['Cluster'],\n",
        "                    cmap='viridis',\n",
        "                    alpha=0.7,\n",
        "                    s=40,\n",
        "                    edgecolors='w',\n",
        "                    linewidth=0.3)\n",
        "\n",
        "ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.2%} varianza)')\n",
        "ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.2%} varianza)')\n",
        "ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.2%} varianza)')\n",
        "ax.set_title('Clustering en Espacio PCA 3D - 4 Clusters\\n(Datos sin outliers)')\n",
        "ax.view_init(elev=25, azim=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualizaciones 2D complementarias\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# PC1 vs PC2\n",
        "axes[0,0].scatter(df_pca_3d['PC1'], df_pca_3d['PC2'],\n",
        "                 c=df_pca_3d['Cluster'], cmap='viridis', alpha=0.7, s=30)\n",
        "axes[0,0].set_xlabel('PC1')\n",
        "axes[0,0].set_ylabel('PC2')\n",
        "axes[0,0].set_title('Vista PC1-PC2')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# PC1 vs PC3\n",
        "axes[0,1].scatter(df_pca_3d['PC1'], df_pca_3d['PC3'],\n",
        "                 c=df_pca_3d['Cluster'], cmap='viridis', alpha=0.7, s=30)\n",
        "axes[0,1].set_xlabel('PC1')\n",
        "axes[0,1].set_ylabel('PC3')\n",
        "axes[0,1].set_title('Vista PC1-PC3')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# PC2 vs PC3\n",
        "axes[1,0].scatter(df_pca_3d['PC2'], df_pca_3d['PC3'],\n",
        "                 c=df_pca_3d['Cluster'], cmap='viridis', alpha=0.7, s=30)\n",
        "axes[1,0].set_xlabel('PC2')\n",
        "axes[1,0].set_ylabel('PC3')\n",
        "axes[1,0].set_title('Vista PC2-PC3')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distribucion de clusters\n",
        "axes[1,1].bar(range(4), cluster_sizes,\n",
        "              color=plt.cm.viridis(np.linspace(0, 1, 4)))\n",
        "axes[1,1].set_xlabel('Cluster')\n",
        "axes[1,1].set_ylabel('N√∫mero de muestras')\n",
        "axes[1,1].set_title('Distribuci√≥n de Clusters')\n",
        "axes[1,1].set_xticks(range(4))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Vistas 2D y Distribuci√≥n de Clusters en Espacio PCA 3D',\n",
        "             fontsize=16, y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I-c4raL01l68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Conclusiones en tres dimensiones\n",
        "**Respuesta**\n",
        "La visualizaci√≥n en 3D muestra que los 4 cl√∫sters no son perfectamente separables, pero s√≠ presentan cierta diferenciaci√≥n en distintas regiones del espacio PCA: un cl√∫ster dominante concentra la mayor√≠a de los pasajeros con caracter√≠sticas promedio, mientras que los otros tres se ubican en zonas m√°s espec√≠ficas y con menor tama√±o, lo que indica la existencia de segmentos minoritarios con comportamientos distintos. Aunque persiste solapamiento entre grupos, esta segmentaci√≥n en 4 cl√∫sters resulta √∫til para identificar patrones generales y subgrupos de inter√©s que pueden aprovecharse en estrategias de marketing m√°s enfocadas.\n"
      ],
      "metadata": {
        "id": "NkODM_L73LEH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8igIgDwpq9mG"
      },
      "source": [
        "Mucho √©xito!\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/55/3d/42/553d42bea9b10e0662a05aa8726fc7f4.gif\" width=300>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "7cb425aec99b4079954fd707109c42c3",
    "deepnote_persisted_session": {
      "createdAt": "2024-04-26T06:15:51.197Z"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}